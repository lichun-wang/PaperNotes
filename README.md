# PaperNotes
## ==阅读整理，记录到Excel==

**==2023年的阅读目标，多探索WHY，多从代码角度理解论文==**

## 待阅读的论文积累：

#### 分类：

*   [ ] Wu, S., Zhang, H. R., & Ré, C. (2020). Understanding and improving information transfer in multi-task learning. arXiv preprint arXiv:2005.00944

*   [x] CoAtNet: Marrying Convolution and Attention for All Data Sizes

*   [ ] NFNet

*   [x] ~~VIT~~

*   [x] Swin transformer

*   [ ] Regularization in ResNet with stochastic Depth (NIPS2021)

*   [x] Deep networks with stochastic depth

*   [ ] Synthesizer: ==Rethinking self-attention for transformer models.==

*   [x] Patches Are all you need ?

*   [x] DeiT

*   [x] CaiT(  Deit的升级版)

*   [x] Mix-MLP

*   [ ] ResMLP

*   [x] FixEfficientNet

*   [x] FixRes

*   [x] BeiT

*   [x] CVT

*   [ ] BiT

*   [ ] ConVit

*   [ ] Crossvit

*   [x] LeViT (轻量)

*   [ ] PVT: Pyramid Vision Transformer

*   [ ] TNT (transformer in transformer)

*   [ ] Meta Pseudo Label

*   [ ] Scaling Vision Transformers

*   [ ] Zero-Shot Text-to-Image Generation

*   [ ] HRformer

*   [ ] Can Vision Transformers perform convolution? transform可以替代卷积，<https://arxiv.org/abs/2111.01353>

*   [x] VOLO

*   [ ] LV-Vit

*   [ ] 讲图像bias的一篇文章：《ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness》，iclr2021《Shape-Texture Debiased Neural Network Training》

*   [ ] EoA方法：Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalizationen

*   [ ] 判断域差别指标： towards non-iid image classification a dataset and baselines

*   [ ] RegNet

*   [ ]

#### 检测：

*   [ ] TPH-YOLOv5
*   [x] ~~YOLOX~~
*   [ ] anchor DETR， <https://arxiv.org/abs/2109.07107>
*   [ ] DETR：
*   [ ] Deformable DETR
*   [ ] OTA：Optimal transport assignment for object detection

#### 视频分类

*   [ ] 20210913：TimeSformer
*   [ ] 20211018: iccv 2021 oral AdaFocus

#### 半监督

*   [ ] 20210912：self-tuning:

*   [x] ~~20210922：fixmatch~~

*   [ ] 20210922：mixmatch

*   [ ] 20210922：remixmatch

*   [x] ~~20210922:   noisy student~~

*   [ ] 20210922:  Dash: Semi-Supervised Learning with Dynamic Thresholding

*   [ ] 20210920:  CSSL: Credal Self0supervised learning

*   [ ] 20211019:  Self-Supervised Learning with Swin Transformers

*   [x] 20211111： ==Masked Autoencoders Are Scalable Vision Learners（He Kaiming）==

*   [ ] 20211209: plabel

#### 自监督

*   [ ] Emerging properties in self-supervised vision transformers
*   [ ] An empirical study of training self-supervised vision transformers (beit文章中看到)
*   [ ] Self-supervised learning with swin transformers（beit文章中看到）
*   [x] moco
*   [x] moco v2
*   [x] moco v3
*   [x] simclr
*   [x] simclr v2
*   [x] Dino , facebook
*   [ ] iGPT
*   [x] BYOL
*   [ ] SEER
*   [ ] SwAV

#### 分割：

*   [ ] &#x20;seam, 自监督分割

#### 量化

*   [ ] Exponential discretization of weights of neural network connections in pre-trained neural networks
*   [ ] Diversifying Sample Generation for Accurate Data-Free Quantization

#### 语义分割

*   \[   ]  弱监督语义分割：<https://arxiv.org/abs/2112.08996>

#### 人脸

*   [ ] 20210913：tinaface

#### OCR

*   [ ] East

*   [ ] Sast

#### NLP：

*   [ ] sentencepiece <https://arxiv.org/pdf/1808.06226.pdf>

#### 长尾分布

*   [ ] 综述

#### 语音识别

*   [ ] [Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition](https://paperswithcode.com/paper/pushing-the-limits-of-semi-supervised)
*   [ ] [Self-training and Pre-training are Complementary for Speech Recognition](https://paperswithcode.com/paper/self-training-and-pre-training-are)
*   [ ] [ASAPP-ASR: Multistream CNN and Self-Attentive SRU for SOTA Speech Recognition](https://paperswithcode.com/paper/asapp-asr-multistream-cnn-and-self-attentive)

#### zero shot

*   [ ] zero-shot text-to-image generation(dall-e)

#### 人体

*   [ ] <https://github.com/PeikeLi/Self-Correction-Human-Parsing>

#### NLP

*   [ ] GPT
*   [ ] GPT2
*   [ ] GPT3
*   [ ] RETRO